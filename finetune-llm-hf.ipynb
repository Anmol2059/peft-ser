{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Updating packages","metadata":{}},{"cell_type":"code","source":"!pip install bitsandbytes\n!pip install accelerate\n!pip install --upgrade transformers\n!pip install --upgrade peft\n!pip install --upgrade datasets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2024-01-06T10:07:01.163316Z","iopub.execute_input":"2024-01-06T10:07:01.16404Z","iopub.status.idle":"2024-01-06T10:07:03.875425Z","shell.execute_reply.started":"2024-01-06T10:07:01.163993Z","shell.execute_reply":"2024-01-06T10:07:03.874351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading Model and Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", padding_side=\"right\",)\ntokenizer.pad_token = tokenizer.eos_token\nbnb_config = BitsAndBytesConfig(\n   load_in_8bit=True,\n#    bnb_4bit_quant_type=\"nf4\",\n#    bnb_4bit_use_double_quant=True,\n   bnb_8bit_compute_dtype=torch.bfloat16\n)\nmodel = AutoModelForCausalLM.from_pretrained(\"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\", device_map=\"auto\", quantization_config=bnb_config)","metadata":{"execution":{"iopub.status.busy":"2024-01-06T10:07:05.744625Z","iopub.execute_input":"2024-01-06T10:07:05.7456Z","iopub.status.idle":"2024-01-06T10:07:14.360496Z","shell.execute_reply.started":"2024-01-06T10:07:05.745561Z","shell.execute_reply":"2024-01-06T10:07:14.35942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"txt = \"\"\"###SYSTEM: Based on INPUT title generate the prompt for generative model\n\n###INPUT: Linux Terminal\n\n###PROMPT:\"\"\"\ntokens = tokenizer(txt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\nop = model.generate(tokens, max_new_tokens=200)\nprint(tokenizer.decode(op[0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing PEFT model","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n\nmodel.gradient_checkpointing_enable()\nmodel = prepare_model_for_kbit_training(model)\n\npeft_config = LoraConfig(inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1, peft_type=TaskType.CAUSAL_LM)\nmodel = get_peft_model(model, peft_config)\n\nprint(model.print_trainable_parameters())\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preparing Dataset","metadata":{}},{"cell_type":"code","source":"def format_dataset(data_point):\n    prompt = f\"\"\"###SYSTEM: Based on INPUT title generate the prompt for generative model\n\n###INPUT: {data_point['act']}\n\n###PROMPT: {data_point['prompt']}\n\"\"\"\n    tokens = tokenizer(prompt,\n        truncation=True,\n        max_length=256,\n        padding=\"max_length\",)\n    tokens[\"labels\"] = tokens['input_ids'].copy()\n    return tokens\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"fka/awesome-chatgpt-prompts\", split=\"train\")\nprint(dataset[0].keys())\n\ndataset = dataset.map(format_dataset)\nprint(dataset[0].keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(dataset[0]['input_ids']))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = dataset.remove_columns(['act', \"prompt\"])\nprint(dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tmp = dataset.train_test_split(test_size=0.1)\ntrain_dataset = tmp[\"train\"]\ntest_dataset = tmp[\"test\"]\nprint(train)\nprint(test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nif torch.cuda.device_count() > 1: \n    model.is_parallelizable = True\n    model.model_parallel = True","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n\ntrainer = Trainer(\n                    model = model, \n                    train_dataset=train_dataset, \n                    eval_dataset = test_dataset,\n                    tokenizer = tokenizer, \n                    data_collator = data_collator, \n\n                    args = TrainingArguments(\n                        output_dir=\"./training\",\n                        remove_unused_columns=False,\n                        per_device_train_batch_size=2,\n                        gradient_checkpointing=True,\n                        gradient_accumulation_steps=4,\n                        max_steps=200,\n                        learning_rate=2.5e-5, \n                        logging_steps=5,\n                        fp16=True,\n                        optim=\"paged_adamw_8bit\",\n                        save_strategy=\"steps\",     \n                        save_steps=50,             \n                        evaluation_strategy=\"steps\",\n                        eval_steps=5,              \n                        do_eval=True,\n                        label_names = [\"input_ids\", \"labels\", \"attention_mask\"],\n                        report_to = \"none\",\n                        \n                ))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generating Text","metadata":{}},{"cell_type":"code","source":"txt = \"\"\"###SYSTEM: Based on INPUT title generate the prompt for generative model\n\n###INPUT: Math Tutor\n\n###PROMPT:\"\"\"\ntokens = tokenizer(txt, return_tensors=\"pt\")['input_ids'].to(\"cuda\")\nop = model.generate(tokens, max_new_tokens=200)\nprint(tokenizer.decode(op[0]))","metadata":{"execution":{"iopub.status.busy":"2024-01-06T10:07:57.548821Z","iopub.execute_input":"2024-01-06T10:07:57.549217Z","iopub.status.idle":"2024-01-06T10:08:31.551695Z","shell.execute_reply.started":"2024-01-06T10:07:57.549184Z","shell.execute_reply":"2024-01-06T10:08:31.550656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Saving PEFT model lora","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"prompt_250_steps\", safe_serialization=False, )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r prompt_250.zip '/kaggle/working/prompt_250_steps' ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading PEFT model weights","metadata":{}},{"cell_type":"code","source":"from peft import PeftModel\nmodel = PeftModel.from_pretrained(model, \"/kaggle/working/prompt_250_steps\")","metadata":{"execution":{"iopub.status.busy":"2024-01-06T10:07:51.768364Z","iopub.execute_input":"2024-01-06T10:07:51.769447Z","iopub.status.idle":"2024-01-06T10:07:51.926983Z","shell.execute_reply.started":"2024-01-06T10:07:51.769412Z","shell.execute_reply":"2024-01-06T10:07:51.926162Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Merging PEFT Weights into Base model and saving","metadata":{}},{"cell_type":"code","source":"model_ = model.merge_and_unload()\nmodel_.save_pretrained(\"merged_model_\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}